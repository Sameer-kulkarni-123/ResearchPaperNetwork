["attribution", "Google", "grants", "permission", "tables", "figures", "journalistic", "scholarly works", "Ashish", "Vaswani", "usz@google.comLlion Jones", "Torontoaidan@cs.toronto.edu", "Polosukhin", "Abstract", "dominant", "models", "complex", "recurrent", "convolutional", "neural networks", "encoder", "decoder", "performing models", "mechanism", "network", "architecture", "Transformer", "attention", "mechanisms", "dispensing", "recurrence", "convolutionsentirely", "Experiments", "machine", "translation", "superior", "quality", "parallelizable", "train", "model", "BLEU", "WMT", "to-German translation task", "improving", "results", "ensembles", "English-to-French translation task", "single-model", "training", "days", "GPUs", "training costs", "literature", "generalizes", "English", "constituency", "training data", "contribution", "random", "Jakob", "RNNs", "self-attention", "evaluate", "Illia", "designed", "crucially", "scaled", "multi-headattention", "parameter-free position", "person", "Niki", "implemented", "evaluated", "variants", "codebase", "Llion", "inference", "visualizations", "Lukasz", "Aidan", "designing", "parts", "tensor2tensor", "research", "Google Brain", "Google Research", "Neural Information Processing Systems", "NIPS", "Long Beach", "CA", "USA", "CL", "short-term", "memory", "gated recurrent", "sequence modeling", "transduction", "problems", "language", "modeling", "push", "boundaries", "language models", "encoder-decoderarchitectures", "Recurrent models", "factor computation", "symbol", "input", "outputsequences", "computation", "sequence", "hidden", "states", "function", "hidden state ht1", "position", "sequential", "nature", "precludes", "parallelization", "training examples", "lengths", "batching", "examples", "improvements", "computational", "efficiency", "factorization tricks", "conditional", "performance", "case", "fundamental", "Attention", "integral", "compelling sequence modeling", "tasks", "dependencies", "distance", "output sequences", "cases", "conjunction", "output", "translation quality", "P100", "goal", "Extended", "Neural GPU", "ConvS2S", "hidden representations", "positions", "operations", "relate", "signals", "arbitrary", "logarithmically", "ByteNet", "distant", "reduced", "constant", "cost", "effective", "resolution", "attention-weighted", "effect", "Multi-Head Attention", "section", "Self-attention", "intra-attention", "representation", "reading comprehension", "abstractive", "summarization", "learning", "End-to-end", "networks", "simple-language question", "answering", "knowledge", "representations", "convolution", "sections", "motivate", "competitive", "neural sequence transduction", "encoder-decoder structure", "maps", "symbol representations", "sequence", "continuous representations", "outputsequence", "symbols", "element", "time", "auto-regressive", "consuming", "point-wise", "connected layers", "left", "right halves", "Figure 1", "Decoder Stacks", "Encoder", "stack", "N", "identical", "layers", "layer", "sub-layers", "residual connection", "normalization", "sub-layer", "Sublayer(x", "Sublayer(x)", "sub-layeritself", "residual", "connections", "outputs", "dimension", "dmodel", "inserts", "modify", "self-attention", "prevent", "attending", "output embeddings", "ensures", "predictions", "i.", "attention function", "mapping", "query", "key-value", "keys", "values", "vectors", "weighted sum3\fScaled Dot-Product AttentionMulti-Head AttentionFigure 2", "weight", "assigned", "compatibility function", "Scaled Dot-Product Attention", "queries", "products", "divide", "softmax function", "weights", "simultaneously", "matrix Q.", "matrices K", "V", "matrix", "Attention(Q", "K", "attention functions", "dot-product", "Dot-product attention", "algorithm", "scaling factor", "computes", "feed-forward network", "hidden layer", "theoretical complexity", "dot-product attention", "faster", "space-efficient", "practice", "code", "small values", "scaling", "grow large", "magnitude", "regions", "gradients", "dmodel-dimensional keys", "learned", "dimensions", "projected versions", "dv-dimensional4To", "components", "Pdki=1 qiki", "variance", "output values", "once again", "depicted", "Figure 2", "attend", "information", "inhibits", "MultiHead(Q", "Concat(head1", "headi", "Attention(QW", "Qi", "KW Ki", "projections", "parameter matrices", "W Ki", "W Vi", "Rdmodel", "Rhdv", "heads", "head", "single-head", "dimensionality", "Applications", "Model", "multi-head", "encoder-decoder attention", "memory keys", "come", "input sequence", "mimics", "sequence-to-sequence models", "self-attention layers", "self-attention layer", "values", "leftward", "auto-regressive property", "implement", "softmax", "illegal", "Position-wise Feed-Forward Networks", "separately", "identically", "linear transformations", "ReLU", "activation", "max(0", "xW1 + b1)W2 + b2(2)", "parameters", "convolutions", "kernel size 1", "inner-layer", "Softmax", "sequence transduction", "tokens", "mation", "next-token", "probabilities", "weight matrix", "pre-softmax", "linear transformation", "Maximum path", "per-layer", "complexity", "operations", "length", "kernel", "size", "neighborhood", "restricted", "TypeComplexity", "Layer", "SequentialMaximum Path", "LengthOperationsSelf-AttentionO(n2  d)O(1)O(1)RecurrentO(n  d2)O(n)O(n)", "ConvolutionalO(k ", "absolute", "positional encodings", "embeddings", "bottoms", "stacks", "summed", "fixed", "sine", "cosine functions", "frequencies", "sin(pos/100002i/dmodel)PE(pos,2i+1", "pos", "positional", "sinusoid", "wavelengths", "geometric progression", "fixed offset k", "linear function", "learned positional embeddings", "versions", "E", "sinusoidal", "compare", "tional layers", "variable-length sequence", "symbol representations", "hiddenlayer", "Motivating", "desiderata", "parallelized", "measured", "path", "long-range dependencies", "Learning long-range", "factor", "affecting", "learn", "paths forward", "backward signals", "shorter", "paths", "combination", "layer types", "connects", "constant number", "sequentially", "recurrent layer", "recurrent layers", "sequence6\flength", "state-of-the-art models", "machine translations", "word-piece", "byte-pair", "increase", "O(n/r", "investigate", "work", "kernel width k", "pairs", "convolutional layers", "contiguous", "dilated", "increasing", "longest paths", "Convolutional layers", "k. Separable convolutions", "decrease", "complexity", "O(k  n  d + n  d2", "point-wise feed-forward layer", "interpretable models", "distributions", "present", "appendix", "behavior", "syntactic", "semantic structure", "sentences", "Training", "training regime", "Training Data", "Batching", "standard", "English-German", "dataset", "Sentences", "target", "vocabulary", "English-French", "English-French dataset", "M sentences", "split tokens", "word-piece", "Sentence pairs", "approximate sequence length", "sentence", "source", "Schedule", "NVIDIA P100", "base models", "hyperparameters", "paper", "steps", "hours", "models,(described", "step time", "big models", "steps", "Optimizer", "Adam optimizer", "1", "learning", "formula", "lrate", "d0.5model", "step_num  warmup_steps1.5)(3)", "learning rate", "warmup_steps", "decreasing", "proportionally", "inverse square", "regularization", "BLEU scores", "English-to-French newstest2014 tests", "fraction", "training cost", "FLOPs", "PosUnk", "RL", "Deep-Att", "Ensemble", "base model", "Dropout", "normalized", "rate", "Pdrop", "Smoothing", "smoothing", "perplexity", "learns", "unsure", "improves", "accuracy", "BLEU score", "Translation", "English-to-German translation task", "big transformer model", "outperforms", "configuration", "bottom line", "Training", "published models", "big model", "published", "state-of-the-art model", "English-to-French", "single model", "checkpoints", "written", "intervals", "beam", "beam size", "length penalty ", "hyperparameters", "experimentation", "development set", "output length", "terminate", "compares", "estimate", "floating", "multiplying", "training time", "sustained", "single-precision floating-point", "capacity", "GPU", "measuring", "English-to-German translation", "TFLOPS", "K80", "K40", "M40", "Variations", "Unlisted values", "basemodel", "metrics", "English-to-German translation development set", "newstest2013", "Listedperplexities", "per-wordpiece", "encoding", "per-word perplexities", "N", "Pdroplstrain", "PPLBLEU", "K", "sinusoids", "A", "attention heads", "Section 3.2.2", "drops", "B", "reducing", "compatibility", "sophisticated", "product", "beneficial", "rows", "C", "D", "dropout", "over-fitting", "row (E", "Parsing", "generalize", "experiments", "constraints", "RNN", "sequence-to-sequencemodels", "attain", "small-data regimes", "4-layer", "transformer", "Wall Street Journal", "portion", "K training sentences", "semi-supervised setting", "high-confidence", "BerkleyParser", "corpora", "M sentences", "K tokens", "WSJ", "K tokens", "learning rates", "Section 22 development set", "unchanged", "English-to-German base translation model", "constituency parsing", "Results", "Section 23of", "ParserTraining", "F1Vinyals", "Petrov", "multi-task", "increased", "maximum", "length + 300", "Table", "task-specific tuning", "prisingly", "exception", "Neural Network Grammar", "Transformer outperforms", "Berkeley-Parser", "WSJ training set", "K sentences", "encoder-decoder architectures", "translation tasks", "architectures", "English-to-German", "English-to-French translation tasks", "excited", "attention-based models", "plan", "modalities", "text", "local", "inputs", "outputs", "images", "audio", "video", "research goals", "ours", "https://github.com/tensorflow/tensor2tensor", "Nal Kalchbrenner", "Gouws", "comments", "corrections", "inspiration", "Jamie Ryan Kiros", "Geoffrey E Hinton", "Layer normalization", "arXiv preprintarXiv:1607.06450", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio", "Neural machine", "align", "translate", "CoRR", "abs/1409.0473", "Denny", "Britz", "Anna Goldie", "Minh-Thang", "Quoc", "exploration", "neuralmachine", "Jianpeng Cheng", "Li", "Mirella Lapata", "memory-networks", "machine", "arXiv preprint arXiv:1601.06733", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Learning phrase representations", "encoder-decoder", "statistical", "Francois Chollet", "Xception", "Deep learning", "arXivpreprint arXiv:1610.02357", "Junyoung Chung", "aglar Glehre", "Empirical evaluation", "gated", "Chris Dyer", "Adhiguna", "Kuncoro", "Miguel Ballesteros", "Noah A. Smith", "neuralnetwork grammars", "NAACL", "Michael Auli", "Denis", "Yarats", "Yann N. Dauphin", "tional sequence", "sequence learning", "arXiv preprint arXiv:1705.03122v2", "Generating sequences", "Xiangyu Zhang", "Jian", "Deep residual learning", "IEEE", "Computer Vision", "PatternRecognition", "pages", "Sepp Hochreiter", "Paolo Frasconi", "Jrgen Schmidhuber", "Gradient flow", "difficulty", "long-term", "Neural computation", "Zhongqiang", "Huang", "Mary Harper", "PCFG", "latent", "languages", "Conference", "Empirical Methods", "Natural", "ACL", "Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu", "arXiv preprint arXiv:1602.02410", "ukasz Kaiser", "Samy Bengio", "active memory replace", "NeuralInformation Processing Systems", "ukasz Kaiser and Ilya", "Sutskever", "Neural", "algorithms", "International Conference", "Learning Representations", "ICLR", "Lasse Espeholt", "Karen Simonyan", "Aaron van den Oord", "Ko-ray Kavukcuoglu", "linear time", "arXiv preprint arXiv:1610.10099v2", "Carl Denton", "Hoang", "Alexander", "M. Rush", "International Conference", "Diederik Kingma", "Jimmy Ba", "method", "stochastic", "optimization", "Oleksii Kuchaiev", "Boris Ginsburg", "Factorization tricks", "LSTM", "arXiv preprintarXiv:1703.10722", "Lin", "Minwei Feng", "Cicero Nogueira dos", "Santos", "Mo", "Bing Xiang", "Bowen", "Zhou", "self-attentive", "embedding", "arXiv preprint", "arXiv:1703.03130", "Lukasz Kaiser", "Multi-tasksequence", "arXiv preprint", "Hieu Pham", "Christopher D Manning", "neural machine", "Mary Ann Marcinkiewicz", "Beatrice Santorini", "corpus", "english", "penn", "treebank", "Computational linguistics", "McClosky", "Eugene Charniak", "parsing", "Proceedings", "Human Language Technology", "Ankur Parikh", "Oscar Tckstrm", "decomposable", "Natural Language", "Processing", "Caiming", "Richard Socher", "deep reinforced model", "arXiv preprint arXiv:1705.04304", "Leon Barrett", "Romain Thibaux", "Dan Klein", "Learning accurate", "compact", "interpretable tree annotation", "Computational", "Linguistics", "Annual", "Meeting", "Ofir Press", "Lior Wolf", "arXivpreprint arXiv:1608.05859", "Barry Haddow", "Alexandra Birch", "rare", "wordswith", "subword units", "arXiv preprint arXiv:1508.07909", "Shazeer", "Azalia Mirhoseini", "Krzysztof Maziarz", "Andy Davis", "Geoffrey Hinton", "Jeff Dean", "sparsely-gated", "arXiv preprint arXiv:1701.06538", "Nitish Srivastava", "Alex Krizhevsky", "Ruslan", "Salakhutdi-nov. Dropout", "overfitting", "MachineLearning", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus", "End-to-end memory", "C. Cortes", "M. Sugiyama", "R. Garnett", "editors", "Advances", "Curran", "Quoc VV", "neuralnetworks", "Christian", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew", "Rethinking", "inception", "computer vision", "abs/1512.00567", "Koo", "Hinton", "Grammar", "foreign language", "Quoc V", "Mohammad Norouzi", "WolfgangMacherey", "Maxim Krikun", "Qin Gao", "Klaus Macherey", "Googles", "Bridging", "gap", "human", "arXiv preprintarXiv:1609.08144", "Xuguang Wang", "Peng Li", "Wei Xu", "Deep recurrent models", "Min Zhang", "Jingbo", "Zhu", "Fast", "accurate", "shift-reduce", "constituent", "Volume1", "Long Papers", "long-distance dependencies", "distant dependency", "phrase", "difficult", "", "Attentions", "colors", "color", "anaphora", "Full attentions", "head 5", "Isolated attentions", "word", "attentions", "behaviour", "structure"]