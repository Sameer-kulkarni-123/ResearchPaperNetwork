["dominant", "models", "complex", "recurrent", "convolutional", "neural networks", "encoder", "decoder", "performing models", "encoder", "decoder", "mechanism", "network", "architecture", "Transformer", "attention", "mechanisms", "dispensing", "recurrence", "convolutions\nentirely", "Experiments", "machine", "translation", "models", "superior", "quality", "parallelizable", "train", "model", "BLEU", "WMT", "to-German translation task", "improving", "results", "ensembles", "BLEU", "WMT", "English-to-French translation task", "model", "single-model", "training", "days", "GPUs", "training costs", "models", "literature", "Transformer", "generalizes", "English", "constituency", "training data", "contribution", "random", "Jakob", "RNNs", "self-attention", "evaluate", "Ashish", "Illia", "designed", "Transformer", "models", "crucially", "scaled", "multi-head\nattention", "parameter-free position", "person", "Niki", "implemented", "evaluated", "variants", "codebase", "Llion", "model", "variants", "codebase", "inference", "visualizations", "Lukasz", "Aidan", "designing", "parts", "tensor2tensor", "codebase", "results", "research", "Google Brain", "Google Research", "Neural Information Processing Systems", "NIPS", "Long Beach", "CA", "USA", "CL"]