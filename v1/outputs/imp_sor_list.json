[[" encoder", "decoder", "opposite of"], ["decoder", "encoder", "opposite of"], [" Listing order", "random", "facet of"], [" self-attention", "RNNs", "subclass of"], [" Transformer", "Illia", "designed by"], [" Niki", "tensor2tensor", "notable work"], ["tensor2tensor", "Niki", "developer"], [" Llion", "codebase", "instance of"], ["Google", "Google Brain", "subsidiary"], [" Long Beach", "USA", "country"], ["CA", "USA", "country"], ["USA", "CA", "contains administrative territorial entity"], [" P100", "GPUs", "subclass of"], [" Multi-Head Attention", "Transformer", "part of"], [" Self-attention", "attention", "subclass of"], [" auto-regressive", "model", "subclass of"], [" Transformer", "encoder", "has part"], ["decoder", "has part", "encoder"], ["Transformer", "part of", "decoder"], ["encoder", "used by", "layer normalization"], ["input sequence", "decoder", "used by"], [" self-attention layer", "encoder", "subclass of"], [" self-attention layer", "decoder", "part of"], ["decoder", "self-attention layer", "has part"], ["decoder", "layers", "subclass of"], ["decoder", "stack", "part of"], ["stack", "encoder", "has part"], [" encoder", "decoder", "different from"], ["decoder", "encoder", "different from"], [" desiderata", "self-attention", "facet of"], ["layer types", "network", "part of"], [" computational performance", "self-attention", "has part"], [" self-attention", "models", "instance of"], [" attention distribution", "models", "facet of"], [" NVIDIA P100", "GPUs", "subclass of"], [" English-to-German", "BLEU", "instance of"], ["English-to-French newstest2014", "BLEU", "instance of"], [" GPUs", "GPUs", "different from"], ["GPUs", "GPUs", "different from"], [" P100", "GPUs", "subclass of"], [" ensemble", "models", "subclass of"], [" GPU 5", "GPUs", "subclass of"], [" Transformer", "architecture", "instance of"], [" best setting", "BLEU", "facet of"], [" Recurrent Neural Network Grammar", "models", "instance of"], ["convolutional", "layers", "subclass of"], [" Attention-based model", "models", "subclass of"], ["self-attentionat layer 5", "encoder", "part of"]]