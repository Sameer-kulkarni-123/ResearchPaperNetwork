["attribution", "Google", "grants", "permission", "tables", "figures", "journalistic", "scholarly works", "Ashish", "Vaswani\u2217\n", "usz@google.com\nLlion Jones\u2217\n", "Toronto\naidan@cs.toronto.edu\n", "Polosukhin\u2217\u2021\n", "Abstract\n", "dominant", "models", "complex", "recurrent", "convolutional", "neural networks", "encoder", "decoder", "performing models", "encoder", "decoder", "mechanism", "network", "architecture", "Transformer", "attention", "mechanisms", "dispensing", "recurrence", "convolutions\nentirely", "Experiments", "machine", "translation", "models", "superior", "quality", "parallelizable", "train", "model", "BLEU", "WMT", "to-German translation task", "improving", "results", "ensembles", "BLEU", "WMT", "English-to-French translation task", "model", "single-model", "training", "days", "GPUs", "training costs", "models", "literature", "Transformer", "generalizes", "English", "constituency", "training data", "contribution", "random", "Jakob", "RNNs", "self-attention", "evaluate", "Ashish", "Illia", "designed", "Transformer", "models", "crucially", "scaled", "multi-head\nattention", "parameter-free position", "person", "Niki", "implemented", "evaluated", "variants", "codebase", "Llion", "model", "variants", "codebase", "inference", "visualizations", "Lukasz", "Aidan", "designing", "parts", "tensor2tensor", "codebase", "results", "research", "Google Brain", "Google Research", "Neural Information Processing Systems", "NIPS", "Long Beach", "CA", "USA", "CL", "neural networks", "short-term", "memory", "gated recurrent", "neural networks", "sequence modeling", "transduction", "problems", "language", "modeling", "machine", "translation", "push", "boundaries", "recurrent", "language models", "encoder-decoder\narchitectures", "Recurrent models", "factor computation", "symbol", "input", "output\nsequences", "computation", "sequence", "hidden\n", "states", "function", "hidden state ht\u22121", "position", "sequential", "nature", "precludes", "parallelization", "training examples", "lengths", "memory", "batching", "examples", "improvements", "computational", "efficiency", "factorization tricks", "conditional", "model", "performance", "case", "fundamental\n", "sequential", "computation", "Attention", "mechanisms", "integral", "compelling sequence modeling", "tasks", "modeling", "dependencies", "distance", "input", "output sequences", "cases", "mechanisms", "conjunction", "recurrent", "network", "Transformer", "model", "architecture", "recurrence", "attention", "mechanism", "input", "output", "Transformer", "parallelization", "translation quality", "P100", "GPUs", "goal", "sequential", "computation", "Extended", "Neural GPU\n", "ConvS2S", "convolutional", "neural networks", "hidden representations", "input", "output", "positions", "models", "operations", "relate", "signals", "arbitrary", "input", "output", "positions", "distance", "positions", "ConvS2S", "logarithmically", "ByteNet", "distant", "positions", "Transformer", "reduced", "constant", "operations", "cost", "reduced", "effective", "resolution", "attention-weighted", "effect", "Multi-Head Attention", "section", "Self-attention", "intra-attention", "attention", "mechanism", "representation", "sequence", "Self-attention", "tasks", "reading comprehension", "abstractive", "summarization", "learning", "End-to-end", "networks", "mechanism", "recurrence", "simple-language question", "answering", "knowledge", "Transformer", "transduction", "model", "self-attention", "representations", "input", "output", "RNNs", "convolution", "sections", "Transformer", "motivate\n", "models", "competitive", "neural sequence transduction", "models", "encoder-decoder structure", "encoder", "maps", "symbol representations", "sequence\n", "continuous representations", "output\nsequence", "symbols", "element", "time", "model", "auto-regressive\n", "consuming", "symbols", "Transformer", "model", "architecture", "Transformer", "architecture", "point-wise", "connected layers", "encoder", "decoder", "left", "right halves", "Figure 1", "Decoder Stacks\n", "Encoder", "encoder", "stack", "N", "identical", "layers", "layer", "sub-layers", "network", "residual connection", "sub-layers", "layer", "normalization", "output", "sub-layer", "Sublayer(x", "Sublayer(x)", "function", "implemented", "sub-layer\nitself", "residual", "connections", "sub-layers", "model", "outputs", "dimension", "dmodel", "decoder", "stack", "N", "identical", "layers", "sub-layers", "encoder", "layer", "inserts", "sub-layer", "output", "encoder", "stack", "encoder", "residual", "connections", "sub-layers", "layer", "normalization", "modify", "self-attention\n", "sub-layer", "stack", "prevent", "positions", "attending", "positions", "output embeddings", "position", "ensures", "predictions", "outputs", "i.\n", "attention function", "mapping", "query", "key-value", "output", "query", "keys", "values", "output", "vectors", "output", "weighted sum\n3\nScaled Dot-Product Attention\nMulti-Head Attention\nFigure 2", "values", "weight", "assigned", "compatibility function", "Scaled Dot-Product Attention", "queries", "keys", "dimension", "values", "dimension", "products", "keys", "divide", "softmax function", "weights", "attention function", "queries", "simultaneously", "matrix Q.", "keys", "values", "matrices K", "V", "matrix", "outputs", "Attention(Q", "K", "attention functions", "dot-product", "Dot-product attention", "algorithm", "scaling factor", "computes", "compatibility function", "feed-forward network", "hidden layer", "theoretical complexity", "dot-product attention", "faster", "space-efficient", "practice", "code", "small values", "mechanisms", "scaling", "values", "grow large", "magnitude", "softmax function", "regions", "gradients", "effect", "attention function", "dmodel-dimensional keys", "values", "queries", "queries", "keys", "values", "learned", "dimensions", "projected versions", "queries", "keys", "values", "attention function", "dv-dimensional\n4To", "components", "random", "Pdk\ni=1 qiki", "variance", "output values", "once again", "values", "depicted", "Figure 2", "model", "attend", "information", "representation", "positions", "inhibits", "MultiHead(Q", "K", "Concat(head1", "headi", "Attention(QW", "Q\ni", "KW K\ni", "projections", "parameter matrices", "W K\ni\n", "W V\ni\n", "\u2208Rdmodel", "\u2208Rhdv", "dmodel", "heads", "reduced", "dimension", "head", "computational", "single-head", "dimensionality", "Applications", "Attention", "Model\n", "Transformer", "multi-head", "encoder-decoder attention", "layers", "queries", "layer", "memory keys", "values", "come", "output", "encoder", "attend", "input sequence", "mimics", "encoder-decoder attention", "mechanisms", "sequence-to-sequence models", "encoder", "self-attention layers", "self-attention layer", "keys", "values\n", "queries", "case", "output", "layer", "encoder", "position", "encoder", "positions", "layer", "encoder", "self-attention layers", "position", "position", "prevent", "leftward", "auto-regressive property", "implement", "scaled", "values", "softmax", "illegal", "connections", "Position-wise Feed-Forward Networks\n", "attention", "sub-layers", "layers", "encoder", "decoder", "network", "position", "separately", "identically", "linear transformations", "ReLU", "activation", "max(0", "xW1 + b1)W2 + b2\n(2)", "linear transformations", "positions", "parameters", "layer", "layer", "convolutions", "kernel size 1", "dimensionality", "input", "output", "dmodel", "inner-layer", "dimensionality", "Softmax\n", "sequence transduction", "models", "learned", "tokens", "vectors", "dimension", "dmodel", "learned", "mation", "softmax function", "next-token", "probabilities", "model", "weight matrix", "layers", "pre-softmax\n", "linear transformation", "layers", "weights", "Maximum path", "lengths", "per-layer", "complexity", "sequential", "operations\n", "layer", "sequence", "length", "representation", "dimension", "kernel\n", "convolutions", "size", "neighborhood", "restricted", "self-attention", "Type\nComplexity", "Layer\n", "Sequential\nMaximum Path", "Length\nOperations\nSelf-Attention\nO(n2 \u00b7 d)\nO(1)\nO(1)\nRecurrent\nO(n \u00b7 d2)\nO(n)\nO(n)\n", "Convolutional\nO(k \u00b7", "restricted", "model", "convolution", "model", "sequence", "information", "absolute", "position", "tokens", "sequence", "positional encodings", "embeddings", "bottoms", "encoder", "decoder", "stacks", "positional encodings", "dimension", "embeddings", "summed", "positional encodings", "fixed", "sine", "cosine functions", "frequencies", "sin(pos/100002i/dmodel)\nPE(pos,2i+1", "pos", "position", "dimension", "dimension", "positional", "sinusoid", "wavelengths", "geometric progression", "function", "model", "fixed offset k", "linear function", "learned positional embeddings", "versions", "identical", "results", "E", "sinusoidal", "model", "sequence", "lengths", "training", "section", "compare", "self-attention layers", "recurrent", "tional layers", "mapping", "variable-length sequence", "symbol representations\n", "sequence", "length", "hidden\nlayer", "sequence transduction", "encoder", "decoder", "Motivating", "self-attention", "desiderata", "computational", "layer", "computation", "parallelized", "measured", "sequential", "operations", "path", "length", "long-range dependencies", "network", "Learning long-range\n", "sequence transduction", "tasks", "factor", "affecting", "learn", "dependencies", "paths forward", "backward signals", "network", "shorter", "paths", "combination", "positions", "input", "output sequences", "length", "input", "output", "networks", "layer types", "self-attention layer", "connects", "constant number", "sequentially\n", "operations", "recurrent layer", "sequential", "operations", "computational", "self-attention layers", "recurrent layers", "sequence\n6\nlength", "representation", "dimensionality", "case", "state-of-the-art models", "machine translations", "word-piece\n", "byte-pair", "representations", "computational", "performance", "tasks", "self-attention", "neighborhood", "size", "input sequence", "output", "position", "increase", "O(n/r", "investigate", "work", "kernel width k", "pairs", "input", "output", "positions", "stack", "convolutional layers", "case", "contiguous", "case", "dilated", "convolutions", "increasing", "length", "longest paths", "network", "Convolutional layers", "factor", "k. Separable convolutions", "decrease", "complexity\n", "O(k \u00b7 n \u00b7 d + n \u00b7 d2", "complexity", "convolution", "combination", "self-attention layer", "point-wise feed-forward layer", "model", "self-attention", "interpretable models", "attention", "distributions", "models", "present", "examples", "appendix", "tasks", "behavior", "syntactic\n", "semantic structure", "sentences", "Training\n", "section", "training regime", "models", "Training Data", "Batching\n", "standard", "WMT", "English-German", "dataset", "Sentences", "byte-pair", "target", "vocabulary", "tokens", "English-French", "WMT", "English-French dataset", "M sentences", "split tokens", "word-piece", "Sentence pairs", "approximate sequence length", "training", "sentence", "pairs", "source", "tokens", "target", "Schedule\n", "models", "machine", "NVIDIA P100", "GPUs", "base models", "hyperparameters", "paper", "training", "base models", "steps", "hours", "models,(described", "step time", "big models", "steps\n", "days", "Optimizer\n", "Adam optimizer", "\u03b21", "learning\n", "training", "formula", "lrate", "d\u22120.5\nmodel", "step_num \u00b7 warmup_steps\u22121.5)\n(3)\n", "increasing", "learning rate", "warmup_steps", "training", "decreasing", "proportionally", "inverse square", "warmup_steps", "regularization", "training", "Transformer", "BLEU scores", "state-of-the-art models", "English-to-French newstest2014 tests", "fraction", "training cost", "FLOPs", "PosUnk", "RL", "Deep-Att", "PosUnk", "Ensemble", "RL", "Ensemble", "base model", "Dropout\n", "output", "sub-layer", "sub-layer", "input", "normalized", "embeddings", "encoder", "decoder", "stacks", "base model", "rate", "Pdrop", "Smoothing\n", "training", "smoothing", "perplexity", "model", "learns", "unsure", "improves", "accuracy", "BLEU score", "Translation\n", "WMT", "English-to-German translation task", "big transformer model", "Transformer", "outperforms", "models", "ensembles", "BLEU", "configuration", "model", "bottom line", "Training", "days", "P100", "GPUs", "base model", "published models", "ensembles", "fraction", "training", "cost", "competitive", "models", "WMT", "English-to-French translation task", "big model", "BLEU score", "published", "training", "cost", "state-of-the-art model", "Transformer", "English-to-French", "Pdrop", "base models", "single model", "checkpoints", "written", "intervals", "big models", "checkpoints", "beam", "beam size", "length penalty \u03b1", "hyperparameters\n", "experimentation", "development set", "output length", "terminate", "results", "compares", "translation quality", "training costs", "model", "literature", "estimate", "floating", "multiplying", "training time", "GPUs", "estimate", "sustained", "single-precision floating-point", "capacity", "GPU", "evaluate", "components", "Transformer", "base model", "measuring", "performance", "English-to-German translation", "values", "TFLOPS", "K80", "K40", "M40", "P100", "Variations", "Transformer", "architecture", "Unlisted values", "identical", "base\nmodel", "metrics", "English-to-German translation development set", "newstest2013", "Listed\nperplexities", "per-wordpiece", "byte-pair", "encoding", "per-word perplexities", "N\n", "Pdrop\n\u03f5ls\ntrain\n", "PPL\nBLEU\n", "K\n", "sinusoids\n", "newstest2013", "beam", "section", "results", "A", "attention heads", "computation", "Section 3.2.2", "single-head", "BLEU", "quality", "drops", "heads", "B", "reducing", "size", "model", "quality", "compatibility", "sophisticated", "compatibility", "product", "beneficial", "rows", "C", "D", "dropout", "over-fitting", "row (E", "learned positional embeddings", "base model", "Parsing\n", "evaluate", "Transformer", "generalize", "experiments", "English", "output", "constraints", "input", "RNN", "sequence-to-sequence\nmodels", "attain", "small-data regimes", "4-layer", "transformer", "dmodel", "Wall Street Journal", "portion", "K training sentences", "semi-supervised setting", "high-confidence", "BerkleyParser", "corpora", "M sentences\n", "vocabulary", "K tokens", "WSJ", "vocabulary", "K tokens\n", "semi-supervised setting", "experiments", "dropout", "attention", "residual", "section", "learning rates", "beam size", "Section 22 development set", "parameters", "unchanged", "English-to-German base translation model", "inference", "Transformer", "generalizes", "English", "constituency parsing", "Results", "Section 23\nof", "Parser\nTraining\n", "F1\nVinyals", "WSJ", "Petrov", "WSJ", "WSJ", "WSJ", "WSJ", "multi-task\n", "increased", "maximum", "output", "length + 300", "beam size", "WSJ", "semi-supervised setting", "results", "Table", "task-specific tuning", "model", "prisingly", "results", "models", "exception", "Neural Network Grammar", "RNN", "sequence-to-sequence models", "Transformer outperforms", "Berkeley-\nParser", "training", "WSJ training set", "K sentences", "Transformer", "sequence transduction", "recurrent layers", "encoder-decoder architectures", "translation tasks", "Transformer", "architectures", "recurrent", "convolutional layers", "WMT", "English-to-German", "WMT", "English-to-French translation tasks", "ensembles", "excited", "attention-based models", "plan", "Transformer", "problems", "input", "output", "modalities", "text", "investigate", "local", "attention", "mechanisms", "inputs", "outputs\n", "images", "audio", "video", "sequential", "research goals", "ours", "code", "train", "evaluate", "models", "https://github.com/\ntensorflow/tensor2tensor", "Nal Kalchbrenner", "Gouws", "comments", "corrections", "inspiration", "Jamie Ryan Kiros", "Geoffrey E Hinton", "Layer normalization", "arXiv preprint\narXiv:1607.06450", "Dzmitry Bahdanau", "Kyunghyun Cho", "Yoshua Bengio", "Neural machine", "translation", "align", "translate", "CoRR", "abs/1409.0473", "Denny", "Britz", "Anna Goldie", "Minh-Thang", "Quoc", "exploration", "neural\nmachine", "CoRR", "Jianpeng Cheng", "Li", "Mirella Lapata", "short-term", "memory-networks", "machine\n", "arXiv preprint arXiv:1601.06733", "Kyunghyun Cho", "Caglar Gulcehre", "Fethi Bougares", "Holger Schwenk", "Yoshua Bengio", "Learning phrase representations", "encoder-decoder", "statistical\n", "translation", "CoRR", "Francois Chollet", "Xception", "Deep learning", "arXiv\npreprint arXiv:1610.02357", "Junyoung Chung", "\u00c7aglar G\u00fcl\u00e7ehre", "Kyunghyun Cho", "Yoshua Bengio", "Empirical evaluation\n", "gated", "neural networks", "sequence modeling", "CoRR", "Chris Dyer", "Adhiguna", "Kuncoro", "Miguel Ballesteros", "Noah A. Smith", "neural\nnetwork grammars", "NAACL", "Michael Auli", "Denis", "Yarats", "Yann N. Dauphin", "tional sequence", "sequence learning", "arXiv preprint arXiv:1705.03122v2", "Generating sequences", "neural networks", "Xiangyu Zhang", "Jian", "Deep residual learning", "IEEE", "Computer Vision", "Pattern\nRecognition", "pages", "Sepp Hochreiter", "Yoshua Bengio", "Paolo Frasconi", "J\u00fcrgen Schmidhuber", "Gradient flow", "difficulty", "learning", "long-term", "dependencies", "Sepp Hochreiter", "J\u00fcrgen Schmidhuber", "short-term", "memory", "Neural computation", "Zhongqiang", "Huang", "Mary Harper", "PCFG", "latent", "languages", "Conference", "Empirical Methods", "Natural\n", "pages", "ACL", "Rafal Jozefowicz", "Oriol Vinyals", "Mike Schuster", "Noam Shazeer", "Yonghui Wu", "language", "modeling", "arXiv preprint arXiv:1602.02410", "\u0141ukasz Kaiser", "Samy Bengio", "active memory replace", "Neural\nInformation Processing Systems", "NIPS", "\u0141ukasz Kaiser and Ilya", "Sutskever", "Neural", "GPUs", "algorithms", "International Conference\n", "Learning Representations", "ICLR", "Lasse Espeholt", "Karen Simonyan", "Aaron van den Oord", "Ko-\nray Kavukcuoglu", "Neural machine", "translation", "linear time", "arXiv preprint arXiv:1610.10099v2", "Carl Denton", "Hoang", "Alexander", "M. Rush", "networks", "International Conference", "Learning Representations", "Diederik Kingma", "Jimmy Ba", "method", "stochastic", "optimization", "ICLR", "Oleksii Kuchaiev", "Boris Ginsburg", "Factorization tricks", "LSTM", "networks", "arXiv preprint\narXiv:1703.10722", "Lin", "Minwei Feng", "Cicero Nogueira dos", "Santos", "Mo", "Bing Xiang", "Bowen\n", "Zhou", "Yoshua Bengio", "self-attentive", "embedding", "arXiv preprint\n", "arXiv:1703.03130", "Minh-Thang", "Quoc", "Oriol Vinyals", "Lukasz Kaiser", "Multi-task\nsequence", "sequence learning", "arXiv preprint", "Minh-Thang", "Hieu Pham", "Christopher D Manning", "neural machine", "translation", "arXiv preprint", "Mary Ann Marcinkiewicz", "Beatrice Santorini", "corpus", "english", "penn", "treebank", "Computational linguistics", "McClosky", "Eugene Charniak", "parsing", "Proceedings", "Human Language Technology", "Conference", "NAACL", "ACL", "Ankur Parikh", "Oscar T\u00e4ckstr\u00f6m", "Jakob", "decomposable", "Empirical Methods", "Natural Language", "Processing", "Caiming", "Richard Socher", "deep reinforced model", "abstractive", "summarization", "arXiv preprint arXiv:1705.04304", "Petrov", "Leon Barrett", "Romain Thibaux", "Dan Klein", "Learning accurate", "compact", "interpretable tree annotation", "Computational", "Linguistics", "Annual", "Meeting", "ACL", "pages", "ACL", "Ofir Press", "Lior Wolf", "language models", "arXiv\npreprint arXiv:1608.05859", "Barry Haddow", "Alexandra Birch", "Neural machine", "translation", "rare", "words\nwith", "subword units", "arXiv preprint arXiv:1508.07909", "Shazeer", "Azalia Mirhoseini", "Krzysztof Maziarz", "Andy Davis", "Quoc", "Geoffrey Hinton", "Jeff Dean", "neural networks", "sparsely-gated", "arXiv preprint arXiv:1701.06538", "Nitish Srivastava", "Geoffrey E Hinton", "Alex Krizhevsky", "Ruslan", "Salakhutdi-\nnov. Dropout", "neural networks", "overfitting", "Machine\nLearning", "Sainbayar Sukhbaatar", "Arthur Szlam", "Rob Fergus", "End-to-end memory\n", "networks", "C. Cortes", "M. Sugiyama", "R. Garnett", "editors", "Advances", "Neural Information Processing Systems", "Curran", "Sutskever", "Oriol Vinyals", "Quoc VV", "sequence learning", "neural\nnetworks", "Neural Information Processing Systems", "pages", "Christian", "Vincent Vanhoucke", "Sergey Ioffe", "Jonathon Shlens", "Zbigniew", "Rethinking", "inception", "architecture", "computer vision", "CoRR", "abs/1512.00567", "Koo", "Petrov", "Sutskever", "Hinton", "Grammar", "foreign language", "Advances", "Neural Information Processing Systems", "Mike Schuster", "Quoc V", "Mohammad Norouzi", "Wolfgang\nMacherey", "Maxim Krikun", "Qin Gao", "Klaus Macherey", "Google\u2019s", "neural machine", "Bridging", "gap", "human", "machine", "translation", "arXiv preprint\narXiv:1609.08144", "Xuguang Wang", "Peng Li", "Wei Xu", "Deep recurrent models", "connections", "neural machine", "translation", "CoRR", "Min Zhang", "Jingbo", "Zhu", "Fast", "accurate", "shift-reduce", "constituent", "Annual", "Meeting", "ACL", "Volume\n1", "Long Papers", "pages", "ACL", "attention", "mechanism", "long-distance dependencies", "self-attention", "layer", "attention", "attend", "distant dependency", "phrase", "difficult", "\u2019", "Attentions", "colors", "heads", "color", "attention", "heads", "layer", "anaphora", "resolution", "Full attentions", "head 5", "Isolated attentions", "word", "attention", "attentions", "word", "attention heads", "behaviour", "structure", "examples", "heads", "encoder", "self-attention\n", "layer", "heads", "learned", "tasks"]